<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Get Started • rDML</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.7.1/clipboard.min.js" integrity="sha384-cV+rhyOuRHc9Ub/91rihWcGmMmCXDeksTtCihMupQHSsi8GIIRDG0ThDc3HGQFJ3" crossorigin="anonymous"></script><!-- sticky kit --><script src="https://cdnjs.cloudflare.com/ajax/libs/sticky-kit/1.1.3/sticky-kit.min.js" integrity="sha256-c4Rlo1ZozqTPE2RLuvbusY3+SU1pQaJC0TjuhygMipw=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Get Started">
<meta property="og:description" content="">
<meta name="twitter:card" content="summary">
<!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">rDML</a>
        <span class="label label-danger" data-toggle="tooltip" data-placement="bottom" title="Unreleased package">0.0.1</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../articles/rDML.html">
    <span class="fa fa-play-circle fa-lg"></span>
     
    Get started
  </a>
</li>
<li>
  <a href="../reference/index.html">
    <span class="fa fa-book fa-lg"></span>
     
    Reference
  </a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-question fa-lg"></span>
     
    Examples
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/examples/distance_clfs.html">Using distance based classifiers</a>
    </li>
    <li>
      <a href="../articles/examples/fitting_dmls.html">Fitting distance metric algorithms</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/jlsuarezdiaz/rDML">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1>Get Started</h1>
                        <h4 class="author">Juan Luis Suárez Díaz</h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/jlsuarezdiaz/rDML/blob/master/vignettes/rDML.Rmd"><code>vignettes/rDML.Rmd</code></a></small>
      <div class="hidden name"><code>rDML.Rmd</code></div>

    </div>

    
    
<div id="what-is-a-distance-metric-learning-algorithm" class="section level2">
<h2 class="hasAnchor">
<a href="#what-is-a-distance-metric-learning-algorithm" class="anchor"></a>What is a distance metric learning algorithm?</h2>
<p>A distance metric learning algorithm (DML) is an algorithm that can learn a similarity measure or distance from the data. This distance can be used for many purposes, such as improving distance based algorithms wither in supervised, semi-supervised or unsupervised learning. DMLs also have interesting applications in dimensionality reduction.</p>
</div>
<div id="how-to-learn-a-distance" class="section level2">
<h2 class="hasAnchor">
<a href="#how-to-learn-a-distance" class="anchor"></a>How to learn a distance</h2>
<p>The (pseudo-)distances learned by distance metric learning algorithms are also known as Mahalanobis distances. This distances are determined by positive semidefinite matrices <span class="math inline">\(M \in \mathcal{M}_d(\mathbb{R})\)</span>, and can be calculated as <span class="math display">\[ d(x,y) = \sqrt{(x-y)^TM(x-y)}, \]</span> for <span class="math inline">\(x, y \in \mathbb{R}^d\)</span>. It is known that the PSD matrix <span class="math inline">\(M\)</span> can be decomposed as <span class="math inline">\(M = L^TL\)</span>, with <span class="math inline">\(L \in \mathcal{M}_d(\mathbb{R})\)</span> is an arbitrary matrix. In this case, we have <span class="math display">\[ d(x,y)^2 = (x-y)^TL^TL(x-y) = (L(x-y))^T(L(x-y)) = \|L(x-y)\|_2^2. \]</span> So every Mahalanobis distance is equivalent to the euclidean distance after applying the linear mapping <span class="math inline">\(L\)</span>.</p>
<p>Matrices <span class="math inline">\(M\)</span> and <span class="math inline">\(L\)</span> define the two approaches for learning a distance. We can either learn the metric matrix <span class="math inline">\(M\)</span> which defines the distance, or learn the linear map <span class="math inline">\(L\)</span>, and calculate the distance in the mapped space. Each DML will learn the distance following one of these approaches.</p>
</div>
<div id="current-algorithms" class="section level2">
<h2 class="hasAnchor">
<a href="#current-algorithms" class="anchor"></a>Current algorithms</h2>
<p>The current available algorithms are:</p>
<ul>
<li><a href="../reference/PCA.html">Principal Component Analysis (PCA)</a></li>
<li><a href="../reference/LDA.html">Linear Discriminant Analysis (LDA)</a></li>
<li><a href="../reference/ANMM.html">Average Neighborhood Margin Maximization (ANMM)</a></li>
<li><a href="../reference/LMNN.html">Large Margin Nearest Neighbors (LMNN)</a></li>
<li><a href="../reference/NCA.html">Neighborhood Component Analysis (NCA)</a></li>
<li><a href="../reference/NCMML.html">Nearest Class Mean Metric Learning (NCMML)</a></li>
<li><a href="../reference/NCMC.html">Nearest Class with Multiple Centroids (NCMC)</a></li>
<li><a href="../reference/ITML.html">Information Theoretic Metric Learning (ITML)</a></li>
<li><a href="../reference/DMLMJ.html">Distance Metric Learning through the Maximization of the Jeffrey divergence (DMLMJ)</a></li>
<li><a href="../reference/MCML.html">Maximally Collapsing Metric Learning (MCML)</a></li>
<li><a href="../reference/LSI.html">Learning with Side Information (LSI)</a></li>
<li><a href="../reference/DML_eig.html">Distance Metric Learning with Eigenvalue Optimization (DML-eig)</a></li>
<li><a href="../reference/LDML.html">Logistic Discriminant Metric Learning (LDML)</a></li>
<li><a href="../reference/KLMNN.html">Kernel Large Margin Nearest Neighbors (KLMNN)</a></li>
<li><a href="../reference/KANMM.html">Kernel Average Neighborhood Margin Maximization (KANMM)</a></li>
<li><a href="../reference/KDMLMJ.html">Kernel Distance Metric Learning through the Maximization of the Jeffrey divergence (KDMLMJ)</a></li>
<li><a href="../reference/KDA.html">Kernel Discriminant Analysis (KDA)</a></li>
</ul>
</div>
<div id="additional-functionalities" class="section level2">
<h2 class="hasAnchor">
<a href="#additional-functionalities" class="anchor"></a>Additional functionalities</h2>
<ul>
<li>Distance based classifiers: <a href="../reference/kNN.html">k-NN + DML</a> and <a href="../reference/NCMC_Classifier.html">NCMC Classifier</a>
</li>
<li>Make plots of distance classifiers with different distances: <a href="../reference/knn_plot.html">knn_plot</a>, <a href="../reference/dml_multiplot.html">dml_multiplot</a> and <a href="../reference/knn_pairplots.html">knn_pairplots</a>
</li>
<li>Parameters estimation with cross validation: <a href="../reference/tune_knn.html">tune_knn</a> and <a href="../reference/tune.html">tune</a>.</li>
</ul>
</div>
<div id="examples" class="section level2">
<h2 class="hasAnchor">
<a href="#examples" class="anchor"></a>Examples</h2>
<p>Get started with the <a href="./index.html">following examples</a></p>
</div>
<div id="see-also" class="section level2">
<h2 class="hasAnchor">
<a href="#see-also" class="anchor"></a>See also</h2>
<p>The <a href="https://github.com/jlsuarezdiaz/pyDML/">pyDML software</a>, which is the DML software used by rDML, and its <a href="https://pydml.readthedocs.io/">documentation</a>.</p>
</div>
<div id="references" class="section level2">
<h2 class="hasAnchor">
<a href="#references" class="anchor"></a>References</h2>
<ul>
<li>Fei Wang and Changshui Zhang. “Feature extraction by maximizing the average neighborhood margin”. In: Computer Vision and Pattern Recognition, 2007. CVPR’07. IEEE Conference on. IEEE. 2007, pages 1-8.</li>
<li>Kilian Q Weinberger and Lawrence K Saul. “Distance metric learning for large margin nearest neighbor classification”. In: Journal of Machine Learning Research 10.Feb (2009), pages 207-244.</li>
<li>Jacob Goldberger et al. “Neighbourhood components analysis”. In: Advances in neural information processing systems. 2005, pages 513-520.</li>
<li>Thomas Mensink et al. “Metric learning for large scale image classification: Generalizing to new classes at near-zero cost”. In: Computer Vision–ECCV 2012. Springer, 2012, pages 488-501.</li>
<li>Jason V Davis et al. “Information-theoretic metric learning”. In: Proceedings of the 24th international conference on Machine learning. ACM. 2007, pages 209-216.</li>
<li>Bac Nguyen, Carlos Morell and Bernard De Baets. “Supervised distance metric learning through maximization of the Jeffrey divergence”. In: Pattern Recognition 64 (2017), pages 215-225.</li>
<li>Amir Globerson and Sam T Roweis. “Metric learning by collapsing classes”. In: Advances in neural information processing systems. 2006, pages 451-458.</li>
<li>Eric P Xing et al. “Distance metric learning with application to clustering with side-information”. In: Advances in neural information processing systems. 2003, pages 521-528.</li>
<li>Yiming Ying and Peng Li. “Distance metric learning with eigenvalue optimization”. In: Journal of Machine Learning Research 13.Jan (2012), pages 1-26.</li>
<li>Matthieu Guillaumin, Jakob Verbeek and Cordelia Schmid. “Is that you? Metric learning approaches for face identification”. In: Computer Vision, 2009 IEEE 12th international conference on. IEEE. 2009, pages 498-505.</li>
<li>Sebastian Mika et al. “Fisher discriminant analysis with kernels”. In: Neural networks for signal processing IX, 1999. Proceedings of the 1999 IEEE signal processing society workshop. Ieee. 1999, pages 41-48.</li>
<li>Lorenzo Torresani and Kuang-chih Lee. “Large margin component analysis”. In: Advances in neural information processing systems. 2007, pages 1385-1392.</li>
</ul>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#what-is-a-distance-metric-learning-algorithm">What is a distance metric learning algorithm?</a></li>
      <li><a href="#how-to-learn-a-distance">How to learn a distance</a></li>
      <li><a href="#current-algorithms">Current algorithms</a></li>
      <li><a href="#additional-functionalities">Additional functionalities</a></li>
      <li><a href="#examples">Examples</a></li>
      <li><a href="#see-also">See also</a></li>
      <li><a href="#references">References</a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Juan Luis Suárez.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://pkgdown.r-lib.org/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  

  </body>
</html>
