% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/dml.R
\name{KLMNN}
\alias{KLMNN}
\title{The kernelized version of LMNN.}
\usage{
KLMNN(num_dims = NULL, learning_rate = "adaptive", eta0 = 0.3,
  initial_metric = NULL, max_iter = 100, prec = 1e-08, tol = 1e-08,
  k = 3, mu = 0.5, learn_inc = 1.01, learn_dec = 0.5,
  eta_thres = 1e-14, kernel = "linear", gamma = NULL, degree = 3,
  coef0 = 1, kernel_params = NULL, target_selection = "kernel")
}
\arguments{
\item{num_dims}{Desired value for dimensionality reduction. Ignored if solver is 'SDP'. If NULL, all features will be kept. Integer.}

\item{learning_rate}{Type of learning rate update for gradient descent. Possible values are:
- 'adaptive' : the learning rate will increase if the gradient step is succesful, else it will decrease.
- 'constant' : the learning rate will be constant during all the gradient steps.}

\item{eta0}{The initial value for learning rate.}

\item{initial_metric}{If array or matrix, and solver is SDP, it must be a positive semidefinite matrix with the starting metric (d x d) for gradient descent, where d is the number of features.
If None, euclidean distance will be used. If a string, the following values are allowed:
- 'euclidean' : the euclidean distance.
- 'scale' : a diagonal matrix that normalizes each attribute according to its range will be used.
If solver is 'SGD', then the array or matrix will represent a linear map (d' x d), where d' is the dimension provided in num_dims.}

\item{max_iter}{Maximum number of iterations of gradient descent. Integer.}

\item{prec}{Precision stop criterion (gradient norm). Float.}

\item{tol}{Tolerance stop criterion (difference between two iterations). Float.}

\item{k}{Number of target neighbors to take. If this algorithm is used for nearest neighbors classification, a good choice is
to take k as the number of neighbors. Integer.}

\item{mu}{The weight of the push error in the minimization algorithm. The objective function is composed of a push error, given by the impostors,
with weight mu, and a pull error, given by the target neighbors, with weight (1-mu). It must be between 0.0 and 1.0.}

\item{learn_inc}{Increase factor for learning rate. Ignored if learning_rate is not 'adaptive'. Float.}

\item{learn_dec}{Decrease factor for learning rate. Ignored if learning_rate is not 'adaptive'. Float.}

\item{eta_thres}{A learning rate threshold stop criterion. Float.}

\item{kernel}{Kernel to use. Allowed values are: "linear" | "poly" | "rbf" | "sigmoid" | "cosine" | "precomputed".}

\item{gamma}{Kernel coefficient for rbf, poly and sigmoid kernels. Ignored by other
kernels. Default value is 1/n_features. Float.}

\item{degree}{Degree for poly kernels. Ignored by other kernels. Integer.}

\item{coef0}{Independent term for poly and sigmoid kernels. Ignored by other kernels. Float.}

\item{kernel_params}{Parameters (keyword arguments) and values for kernel passed as
callable object. Ignored by other kernels.}

\item{target_selection}{How to find the target neighbors. Allowed values are:
- 'kernel' : using the euclidean distance in the kernel space.
- 'original' : using the euclidean distance in the original space.}
}
\value{
The KLMNN transformer, structured as a named list.
}
\description{
The kernelized version of LMNN.
}
\references{
Kilian Q Weinberger and Lawrence K Saul. “Distance metric learning for large margin nearest
            neighbor classification”. In: Journal of Machine Learning Research 10.Feb (2009), pages 207-244.

Lorenzo Torresani and Kuang-chih Lee. “Large margin component analysis”. In: Advances in neural
            information processing systems. 2007, pages 1385-1392.
}
