% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/dml.R
\name{KDA}
\alias{KDA}
\title{Kernel Discriminant Analysis (KDA).}
\usage{
KDA(solver = "eigen", n_components = NULL, tol = 1e-04,
  kernel = "linear", gamma = NULL, degree = 3, coef0 = 1,
  kernel_params = NULL)
}
\arguments{
\item{solver}{Solver to use, posible values:
- 'eigen': Eigenvalue decomposition.}

\item{n_components}{Number of components (lower than number of classes -1) for dimensionality reduction. If NULL, classes - 1 is used. Integer.}

\item{tol}{Singularity toleration level. Float.}

\item{kernel}{Kernel to use. Allowed values are: "linear" | "poly" | "rbf" | "sigmoid" | "cosine" | "precomputed".}

\item{gamma}{Kernel coefficient for rbf, poly and sigmoid kernels. Ignored by other
kernels. Default value is 1/n_features. Float.}

\item{degree}{Degree for poly kernels. Ignored by other kernels. Integer.}

\item{coef0}{Independent term for poly and sigmoid kernels. Ignored by other kernels. Float.}

\item{kernel_params}{Parameters (keyword arguments) and values for kernel passed as
callable object. Ignored by other kernels.}
}
\value{
The KDA transformer, structured as a named list.
}
\description{
Discriminant analysis in high dimensionality using the kernel trick.
}
\references{
Sebastian Mika et al. “Fisher discriminant analysis with kernels”. In: Neural networks for signal
            processing IX, 1999. Proceedings of the 1999 IEEE signal processing society workshop. Ieee. 1999,
            pages 41-48.
}
