% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/dml.R
\name{KDMLMJ}
\alias{KDMLMJ}
\title{The kernelized version of DMLMJ.}
\usage{
KDMLMJ(num_dims = NULL, n_neighbors = 3, alpha = 0.001,
  reg_tol = 1e-10, kernel = "linear", gamma = NULL, degree = 3,
  coef0 = 1, kernel_params = NULL)
}
\arguments{
\item{num_dims}{Dimension desired for the transformed data. If NULL, dimension will be the number of features.}

\item{n_neighbors}{Number of neighbors to consider in the computation of the difference spaces.}

\item{alpha}{Regularization parameter for inverse matrix computation.}

\item{reg_tol}{Tolerance threshold for applying regularization. The tolerance is compared with the matrix determinant.}

\item{kernel}{Kernel to use. Allowed values are: "linear" | "poly" | "rbf" | "sigmoid" | "cosine" | "precomputed".}

\item{gamma}{Kernel coefficient for rbf, poly and sigmoid kernels. Ignored by other
kernels. Default value is 1/n_features. Float.}

\item{degree}{Degree for poly kernels. Ignored by other kernels. Integer.}

\item{coef0}{Independent term for poly and sigmoid kernels. Ignored by other kernels. Float.}

\item{kernel_params}{Parameters (keyword arguments) and values for kernel passed as
callable object. Ignored by other kernels.}
}
\value{
The KDMLMJ transformer, structured as a named list.
}
\description{
The kernelized version of DMLMJ.
}
\references{
Bac Nguyen, Carlos Morell and Bernard De Baets. “Supervised distance metric learning through
            maximization of the Jeffrey divergence”. In: Pattern Recognition 64 (2017), pages 215-225.
}
